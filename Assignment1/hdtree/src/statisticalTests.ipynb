{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "# import hdtree\n",
    "from hdtree import HDTreeClassifier\n",
    "from information_measure import EntropyMeasure\n",
    "# import information_measure\n",
    "# import split_rule\n",
    "from split_rule import LessThanHalfOfSplit, SingleCategorySplit, FixedValueSplit, TwentyQuantileSplit, LogisticRegressionSingleSplit, AbstractQuantileSplit, TwentyQuantileRangeSplit, TwoQuantileRangeSplit, LogisticRegressionDoubleCategorySplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, recall_score, precision_score, f1_score, roc_curve, roc_auc_score, plot_roc_curve, auc\n",
    "import pickle\n",
    "from joblib import dump, load\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for t test to evaluate two models\n",
    "def t_test(model1, model2):\n",
    "    # Null hypothesis: model1 and model2 are the same\n",
    "    # model1 and model2 are lists of accuracies\n",
    "    # returns the t statistic and p value\n",
    "    # for the t test\n",
    "    import numpy as np\n",
    "    from scipy import stats\n",
    "    t, p = stats.ttest_ind(model1, model2)\n",
    "    print(\"NULL HYPOTHESIS: model1 and model2 are the same\")\n",
    "    if p < 0.05:\n",
    "        print('Reject null hypothesis')\n",
    "        if(np.mean(model1) > np.mean(model2)):\n",
    "            print('single attribute split model is better than double attribute split model')\n",
    "        else:\n",
    "            print('double attribute split model is better than single attribute split model')\n",
    "    else:\n",
    "        print('Fail to reject null hypothesis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for wilcoxon signed rank test to evaluate two models\n",
    "def wilcoxon_test(model1, model2):\n",
    "    # Null hypothesis: model1 and model2 are the same\n",
    "    # model1 and model2 are lists of accuracies\n",
    "    # returns the t statistic and p value\n",
    "    # for the wilcoxon signed rank test\n",
    "    from scipy.stats import wilcoxon\n",
    "    t, p = wilcoxon(model1, model2)\n",
    "    print(\"NULL HYPOTHESIS: model1 and model2 are the same\")\n",
    "    if p < 0.05:\n",
    "        print('Reject null hypothesis')\n",
    "        if(np.mean(model1) > np.mean(model2)):\n",
    "            print('single attribute split model is better than double attribute split model')\n",
    "        else:\n",
    "            print('double attribute split model is better than single attribute split model')\n",
    "    else:\n",
    "        print('Fail to reject null hypothesis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for mann whitney u test to evaluate two models\n",
    "def mannwhitneyu_test(model1, model2):\n",
    "    # Null hypothesis: model1 and model2 are the same\n",
    "    # model1 and model2 are lists of accuracies\n",
    "    # returns the t statistic and p value\n",
    "    # for the mann whitney u test\n",
    "    from scipy.stats import mannwhitneyu\n",
    "    t, p = mannwhitneyu(model1, model2)\n",
    "    print(\"NULL HYPOTHESIS: model1 and model2 are the same\")\n",
    "    if p < 0.05:\n",
    "        print('Reject null hypothesis')\n",
    "        if(np.mean(model1) > np.mean(model2)):\n",
    "            print('single attribute split model is better than double attribute split model')\n",
    "        else:\n",
    "            print('double attribute split model is better than single attribute split model')\n",
    "    else:\n",
    "        print('Fail to reject null hypothesis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for friedman test to evaluate two models\n",
    "def friedman_test(model1, model2):\n",
    "    # Null hypothesis: model1 and model2 are the same\n",
    "    # model1 and model2 are lists of accuracies\n",
    "    # returns the t statistic and p value\n",
    "    # for the friedman test\n",
    "    from scipy.stats import friedmanchisquare\n",
    "    t, p = friedmanchisquare(model1, model2)\n",
    "    print(\"NULL HYPOTHESIS: model1 and model2 are the same\")\n",
    "    if p < 0.05:\n",
    "        print('Reject null hypothesis')\n",
    "        if(np.mean(model1) > np.mean(model2)):\n",
    "            print('single attribute split model is better than double attribute split model')\n",
    "        else:\n",
    "            print('double attribute split model is better than single attribute split model')\n",
    "    else:\n",
    "        print('Fail to reject null hypothesis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for kruskal wallis test to evaluate two models\n",
    "def kruskalwallis_test(model1, model2):\n",
    "    # Null hypothesis: model1 and model2 are the same\n",
    "    # model1 and model2 are lists of accuracies\n",
    "    # returns the t statistic and p value\n",
    "    # for the kruskal wallis test\n",
    "    from scipy.stats import kruskal\n",
    "    t, p = kruskal(model1, model2)\n",
    "    print(\"NULL HYPOTHESIS: model1 and model2 are the same\")\n",
    "    if p < 0.05:\n",
    "        print('Reject null hypothesis')\n",
    "        if(np.mean(model1) > np.mean(model2)):\n",
    "            print('single attribute split model is better than double attribute split model')\n",
    "        else:\n",
    "            print('double attribute split model is better than single attribute split model')\n",
    "    else:\n",
    "        print('Fail to reject null hypothesis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for chi squared test to evaluate two models\n",
    "def chi_squared_test(model1, model2):\n",
    "    # Null hypothesis: model1 and model2 are the same\n",
    "    # model1 and model2 are lists of accuracies\n",
    "    # returns the t statistic and p value\n",
    "    # for the chi squared test\n",
    "    from scipy.stats import chi2_contingency\n",
    "    t, p = chi2_contingency([model1, model2])\n",
    "    print(\"NULL HYPOTHESIS: model1 and model2 are the same\")\n",
    "    if p < 0.05:\n",
    "        print('Reject null hypothesis')\n",
    "        if(np.mean(model1) > np.mean(model2)):\n",
    "            print('single attribute split model is better than double attribute split model')\n",
    "        else:\n",
    "            print('double attribute split model is better than single attribute split model')\n",
    "    else:\n",
    "        print('Fail to reject null hypothesis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for anova test to evaluate two models\n",
    "def anova_test(model1, model2):\n",
    "    # Null hypothesis: model1 and model2 are the same\n",
    "    # model1 and model2 are lists of accuracies\n",
    "    # returns the t statistic and p value\n",
    "    # for the anova test\n",
    "    from scipy.stats import f_oneway\n",
    "    t, p = f_oneway(model1, model2)\n",
    "    print(\"NULL HYPOTHESIS: model1 and model2 are the same\")\n",
    "    if p < 0.05:\n",
    "        print('Reject null hypothesis')\n",
    "        if(np.mean(model1) > np.mean(model2)):\n",
    "            print('single attribute split model is better than double attribute split model')\n",
    "        else:\n",
    "            print('double attribute split model is better than single attribute split model')\n",
    "    else:\n",
    "        print('Fail to reject null hypothesis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkStatisticalTests(model1, model2):\n",
    "    print('t test results: ')\n",
    "    t_test(model1, model2)\n",
    "    print('wilcoxon test results: ')\n",
    "    wilcoxon_test(model1, model2)\n",
    "    print('mann whitney u test results: ')\n",
    "    mannwhitneyu_test(model1, model2)\n",
    "    print('friedman test results: ')\n",
    "    friedman_test(model1, model2)\n",
    "    print('kruskal wallis test results: ')\n",
    "    kruskalwallis_test(model1, model2)\n",
    "    print('chi squared test results: ')\n",
    "    chi_squared_test(model1, model2)\n",
    "    print('anova test results: ')\n",
    "    anova_test(model1, model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
